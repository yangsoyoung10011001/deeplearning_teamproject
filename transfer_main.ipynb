{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangsoyoung10011001/deeplearning_teamproject/blob/main/transfer_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip dataset"
      ],
      "metadata": {
        "id": "oS7rcoNAW31T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1rFRRvNzLOm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU9rjIGtWiMB"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "file_name='newDataset.zip'\n",
        "zip_path='/content/drive/Shareddrives/딥러닝팀플/a-PyTorch-Tutorial-to-Object-Detection-master/dataset/newDataset.zip'\n",
        "\n",
        "# 현재 디렉토리로 파일 복사\n",
        "!cp '{zip_path}' .\n",
        "# unzip file\n",
        "!unzip -q '{file_name}'\n",
        "# remove file\n",
        "!rm '{file_name}'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils.py"
      ],
      "metadata": {
        "id": "_aNKcXqpwddw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "import torchvision.transforms.functional as FT\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Label map\n",
        "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'policecar', 'firetruck', 'ambulance')\n",
        "# global label_map\n",
        "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
        "label_map['background'] = 0\n",
        "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
        "\n",
        "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
        "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
        "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
        "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
        "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}\n",
        "\n",
        "\n",
        "def parse_annotation(annotation_path):\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    boxes = list()\n",
        "    labels = list()\n",
        "    difficulties = list()\n",
        "    for object in root.iter('object'):\n",
        "\n",
        "        difficult = int(object.find('difficult').text == '1')\n",
        "\n",
        "        label = object.find('name').text.lower().strip()\n",
        "        if label not in label_map:\n",
        "            continue\n",
        "\n",
        "        bbox = object.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text) - 1\n",
        "        ymin = int(bbox.find('ymin').text) - 1\n",
        "        xmax = int(bbox.find('xmax').text) - 1\n",
        "        ymax = int(bbox.find('ymax').text) - 1\n",
        "\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label_map[label])\n",
        "        difficulties.append(difficult)\n",
        "\n",
        "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}\n",
        "\n",
        "\n",
        "def create_data_lists(new_path, output_folder):\n",
        "    \"\"\"\n",
        "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
        "\n",
        "    :param voc07_path: path to the 'VOC2007' folder\n",
        "    :param voc12_path: path to the 'VOC2012' folder\n",
        "    :param output_folder: folder where the JSONs must be saved\n",
        "    \"\"\"\n",
        "    new_path = os.path.abspath(new_path)\n",
        "\n",
        "    train_images = list()\n",
        "    train_objects = list()\n",
        "    n_objects = 0\n",
        "\n",
        "    # Training data\n",
        "    for path in [new_path]:\n",
        "\n",
        "        # Find IDs of images in training data\n",
        "        with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
        "            ids = f.read().splitlines()\n",
        "\n",
        "        for id in ids:\n",
        "            # Parse annotation's XML file\n",
        "            objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
        "            if len(objects['boxes']) == 0:\n",
        "                continue\n",
        "            n_objects += len(objects)\n",
        "            train_objects.append(objects)\n",
        "            train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
        "\n",
        "    assert len(train_objects) == len(train_images)\n",
        "\n",
        "    # Save to file\n",
        "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
        "        json.dump(train_images, j)\n",
        "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
        "        json.dump(train_objects, j)\n",
        "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
        "        json.dump(label_map, j)  # save label map too\n",
        "\n",
        "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
        "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
        "\n",
        "    # Test data\n",
        "    test_images = list()\n",
        "    test_objects = list()\n",
        "    n_objects = 0\n",
        "\n",
        "    # Find IDs of images in the test data\n",
        "    with open(os.path.join(new_path, 'ImageSets/Main/test.txt')) as f:\n",
        "        ids = f.read().splitlines()\n",
        "\n",
        "    for id in ids:\n",
        "        # Parse annotation's XML file\n",
        "        objects = parse_annotation(os.path.join(new_path, 'Annotations', id + '.xml'))\n",
        "        if len(objects) == 0:\n",
        "            continue\n",
        "        test_objects.append(objects)\n",
        "        n_objects += len(objects)\n",
        "        test_images.append(os.path.join(new_path, 'JPEGImages', id + '.jpg'))\n",
        "\n",
        "    assert len(test_objects) == len(test_images)\n",
        "\n",
        "    # Save to file\n",
        "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
        "        json.dump(test_images, j)\n",
        "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
        "        json.dump(test_objects, j)\n",
        "\n",
        "    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n",
        "        len(test_images), n_objects, os.path.abspath(output_folder)))\n",
        "\n",
        "\n",
        "def decimate(tensor, m):\n",
        "    \"\"\"\n",
        "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
        "\n",
        "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
        "\n",
        "    :param tensor: tensor to be decimated\n",
        "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
        "    :return: decimated tensor\n",
        "    \"\"\"\n",
        "    assert tensor.dim() == len(m)\n",
        "    for d in range(tensor.dim()):\n",
        "        if m[d] is not None:\n",
        "            tensor = tensor.index_select(dim=d,\n",
        "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n",
        "    \"\"\"\n",
        "    Calculate the Mean Average Precision (mAP) of detected objects.\n",
        "\n",
        "    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n",
        "\n",
        "    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n",
        "    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n",
        "    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n",
        "    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n",
        "    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n",
        "    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n",
        "    :return: list of average precisions for all classes, mean average precision (mAP)\n",
        "    \"\"\"\n",
        "    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n",
        "        true_labels) == len(\n",
        "        true_difficulties)  # these are all lists of tensors of the same length, i.e. number of images\n",
        "    n_classes = len(label_map)\n",
        "\n",
        "    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n",
        "    true_images = list()\n",
        "    for i in range(len(true_labels)):\n",
        "        true_images.extend([i] * true_labels[i].size(0))\n",
        "    true_images = torch.LongTensor(true_images).to(\n",
        "        device)  # (n_objects), n_objects is the total no. of objects across all images\n",
        "    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n",
        "    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n",
        "    true_difficulties = torch.cat(true_difficulties, dim=0)  # (n_objects)\n",
        "\n",
        "    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n",
        "\n",
        "    # Store all detections in a single continuous tensor while keeping track of the image it is from\n",
        "    det_images = list()\n",
        "    for i in range(len(det_labels)):\n",
        "        det_images.extend([i] * det_labels[i].size(0))\n",
        "    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n",
        "    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n",
        "    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n",
        "    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n",
        "\n",
        "    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n",
        "\n",
        "    # Calculate APs for each class (except background)\n",
        "    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n",
        "    for c in range(1, n_classes):\n",
        "        # Extract only objects with this class\n",
        "        true_class_images = true_images[true_labels == c]  # (n_class_objects)\n",
        "        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n",
        "        true_class_difficulties = true_difficulties[true_labels == c]  # (n_class_objects)\n",
        "        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n",
        "\n",
        "        # Keep track of which true objects with this class have already been 'detected'\n",
        "        # So far, none\n",
        "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n",
        "            device)  # (n_class_objects)\n",
        "\n",
        "        # Extract only detections with this class\n",
        "        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n",
        "        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n",
        "        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n",
        "        n_class_detections = det_class_boxes.size(0)\n",
        "        if n_class_detections == 0:\n",
        "            continue\n",
        "\n",
        "        # Sort detections in decreasing order of confidence/scores\n",
        "        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n",
        "        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n",
        "        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n",
        "\n",
        "        # In the order of decreasing scores, check if true or false positive\n",
        "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
        "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
        "        for d in range(n_class_detections):\n",
        "            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n",
        "            this_image = det_class_images[d]  # (), scalar\n",
        "\n",
        "            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n",
        "            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n",
        "            object_difficulties = true_class_difficulties[true_class_images == this_image]  # (n_class_objects_in_img)\n",
        "            # If no such object in this image, then the detection is a false positive\n",
        "            if object_boxes.size(0) == 0:\n",
        "                false_positives[d] = 1\n",
        "                continue\n",
        "\n",
        "            # Find maximum overlap of this detection with objects in this image of this class\n",
        "            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n",
        "            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n",
        "\n",
        "            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n",
        "            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n",
        "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
        "            # We need 'original_ind' to update 'true_class_boxes_detected'\n",
        "\n",
        "            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n",
        "            if max_overlap.item() > 0.5:\n",
        "                # If the object it matched with is 'difficult', ignore it\n",
        "                if object_difficulties[ind] == 0:\n",
        "                    # If this object has already not been detected, it's a true positive\n",
        "                    if true_class_boxes_detected[original_ind] == 0:\n",
        "                        true_positives[d] = 1\n",
        "                        true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n",
        "                    # Otherwise, it's a false positive (since this object is already accounted for)\n",
        "                    else:\n",
        "                        false_positives[d] = 1\n",
        "            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n",
        "            else:\n",
        "                false_positives[d] = 1\n",
        "\n",
        "        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n",
        "        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n",
        "        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n",
        "        cumul_precision = cumul_true_positives / (\n",
        "                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n",
        "        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n",
        "\n",
        "        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n",
        "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n",
        "        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n",
        "        for i, t in enumerate(recall_thresholds):\n",
        "            recalls_above_t = cumul_recall >= t\n",
        "            if recalls_above_t.any():\n",
        "                precisions[i] = cumul_precision[recalls_above_t].max()\n",
        "            else:\n",
        "                precisions[i] = 0.\n",
        "        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n",
        "\n",
        "    # Calculate Mean Average Precision (mAP)\n",
        "    mean_average_precision = average_precisions.sum().item() / 6\n",
        "\n",
        "    # Keep class-wise average precisions in a dictionary\n",
        "    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n",
        "\n",
        "    return average_precisions, mean_average_precision\n",
        "\n",
        "\n",
        "def xy_to_cxcy(xy):\n",
        "    \"\"\"\n",
        "    Convert bounding boxes from boundary coordinates (x_min, y_min, x_max, y_max) to center-size coordinates (c_x, c_y, w, h).\n",
        "\n",
        "    :param xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
        "    :return: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
        "    \"\"\"\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
        "\n",
        "\n",
        "def cxcy_to_xy(cxcy):\n",
        "    \"\"\"\n",
        "    Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n",
        "\n",
        "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
        "    :return: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
        "    \"\"\"\n",
        "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
        "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
        "\n",
        "\n",
        "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
        "    \"\"\"\n",
        "    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n",
        "\n",
        "    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n",
        "    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n",
        "\n",
        "    In the model, we are predicting bounding box coordinates in this encoded form.\n",
        "\n",
        "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n",
        "    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n",
        "    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n",
        "    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n",
        "    # See https://github.com/weiliu89/caffe/issues/155\n",
        "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
        "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
        "\n",
        "\n",
        "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
        "    \"\"\"\n",
        "    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n",
        "\n",
        "    They are decoded into center-size coordinates.\n",
        "\n",
        "    This is the inverse of the function above.\n",
        "\n",
        "    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n",
        "    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n",
        "    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
        "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h\n",
        "\n",
        "\n",
        "def find_intersection(set_1, set_2):\n",
        "    \"\"\"\n",
        "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
        "\n",
        "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
        "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
        "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
        "    \"\"\"\n",
        "\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
        "\n",
        "\n",
        "def find_jaccard_overlap(set_1, set_2):\n",
        "    \"\"\"\n",
        "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
        "\n",
        "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
        "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
        "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Find intersections\n",
        "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
        "\n",
        "    # Find areas of each box in both sets\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
        "\n",
        "    # Find the union\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
        "\n",
        "    return intersection / union  # (n1, n2)\n",
        "\n",
        "\n",
        "# Some augmentation functions below have been adapted from\n",
        "# From https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
        "\n",
        "def expand(image, boxes, filler):\n",
        "    \"\"\"\n",
        "    Perform a zooming out operation by placing the image in a larger canvas of filler material.\n",
        "\n",
        "    Helps to learn to detect smaller objects.\n",
        "\n",
        "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param filler: RBG values of the filler material, a list like [R, G, B]\n",
        "    :return: expanded image, updated bounding box coordinates\n",
        "    \"\"\"\n",
        "    # Calculate dimensions of proposed expanded (zoomed-out) image\n",
        "    original_h = image.size(1)\n",
        "    original_w = image.size(2)\n",
        "    max_scale = 4\n",
        "    scale = random.uniform(1, max_scale)\n",
        "    new_h = int(scale * original_h)\n",
        "    new_w = int(scale * original_w)\n",
        "\n",
        "    # Create such an image with the filler\n",
        "    filler = torch.FloatTensor(filler)  # (3)\n",
        "    new_image = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(1).unsqueeze(1)  # (3, new_h, new_w)\n",
        "    # Note - do not use expand() like new_image = filler.unsqueeze(1).unsqueeze(1).expand(3, new_h, new_w)\n",
        "    # because all expanded values will share the same memory, so changing one pixel will change all\n",
        "\n",
        "    # Place the original image at random coordinates in this new image (origin at top-left of image)\n",
        "    left = random.randint(0, new_w - original_w)\n",
        "    right = left + original_w\n",
        "    top = random.randint(0, new_h - original_h)\n",
        "    bottom = top + original_h\n",
        "    new_image[:, top:bottom, left:right] = image\n",
        "\n",
        "    # Adjust bounding boxes' coordinates accordingly\n",
        "    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(\n",
        "        0)  # (n_objects, 4), n_objects is the no. of objects in this image\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "\n",
        "def random_crop(image, boxes, labels, difficulties):\n",
        "    \"\"\"\n",
        "    Performs a random crop in the manner stated in the paper. Helps to learn to detect larger and partial objects.\n",
        "\n",
        "    Note that some objects may be cut out entirely.\n",
        "\n",
        "    Adapted from https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
        "\n",
        "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
        "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
        "    :return: cropped image, updated bounding box coordinates, updated labels, updated difficulties\n",
        "    \"\"\"\n",
        "    original_h = image.size(1)\n",
        "    original_w = image.size(2)\n",
        "    # Keep choosing a minimum overlap until a successful crop is made\n",
        "    while True:\n",
        "        # Randomly draw the value for minimum overlap\n",
        "        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])  # 'None' refers to no cropping\n",
        "\n",
        "        # If not cropping\n",
        "        if min_overlap is None:\n",
        "            return image, boxes, labels, difficulties\n",
        "\n",
        "        # Try up to 50 times for this choice of minimum overlap\n",
        "        # This isn't mentioned in the paper, of course, but 50 is chosen in paper authors' original Caffe repo\n",
        "        max_trials = 50\n",
        "        for _ in range(max_trials):\n",
        "            # Crop dimensions must be in [0.3, 1] of original dimensions\n",
        "            # Note - it's [0.1, 1] in the paper, but actually [0.3, 1] in the authors' repo\n",
        "            min_scale = 0.3\n",
        "            scale_h = random.uniform(min_scale, 1)\n",
        "            scale_w = random.uniform(min_scale, 1)\n",
        "            new_h = int(scale_h * original_h)\n",
        "            new_w = int(scale_w * original_w)\n",
        "\n",
        "            # Aspect ratio has to be in [0.5, 2]\n",
        "            aspect_ratio = new_h / new_w\n",
        "            if not 0.5 < aspect_ratio < 2:\n",
        "                continue\n",
        "\n",
        "            # Crop coordinates (origin at top-left of image)\n",
        "            left = random.randint(0, original_w - new_w)\n",
        "            right = left + new_w\n",
        "            top = random.randint(0, original_h - new_h)\n",
        "            bottom = top + new_h\n",
        "            crop = torch.FloatTensor([left, top, right, bottom])  # (4)\n",
        "\n",
        "            # Calculate Jaccard overlap between the crop and the bounding boxes\n",
        "            overlap = find_jaccard_overlap(crop.unsqueeze(0),\n",
        "                                           boxes)  # (1, n_objects), n_objects is the no. of objects in this image\n",
        "            overlap = overlap.squeeze(0)  # (n_objects)\n",
        "\n",
        "            # If not a single bounding box has a Jaccard overlap of greater than the minimum, try again\n",
        "            if overlap.max().item() < min_overlap:\n",
        "                continue\n",
        "\n",
        "            # Crop image\n",
        "            new_image = image[:, top:bottom, left:right]  # (3, new_h, new_w)\n",
        "\n",
        "            # Find centers of original bounding boxes\n",
        "            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.  # (n_objects, 2)\n",
        "\n",
        "            # Find bounding boxes whose centers are in the crop\n",
        "            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (\n",
        "                    bb_centers[:, 1] < bottom)  # (n_objects), a Torch uInt8/Byte tensor, can be used as a boolean index\n",
        "\n",
        "            # If not a single bounding box has its center in the crop, try again\n",
        "            if not centers_in_crop.any():\n",
        "                continue\n",
        "\n",
        "            # Discard bounding boxes that don't meet this criterion\n",
        "            new_boxes = boxes[centers_in_crop, :]\n",
        "            new_labels = labels[centers_in_crop]\n",
        "            new_difficulties = difficulties[centers_in_crop]\n",
        "\n",
        "            # Calculate bounding boxes' new coordinates in the crop\n",
        "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])  # crop[:2] is [left, top]\n",
        "            new_boxes[:, :2] -= crop[:2]\n",
        "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])  # crop[2:] is [right, bottom]\n",
        "            new_boxes[:, 2:] -= crop[:2]\n",
        "\n",
        "            return new_image, new_boxes, new_labels, new_difficulties\n",
        "\n",
        "\n",
        "def horizontal_flip(image, boxes):\n",
        "    \"\"\"\n",
        "    Flip image horizontally.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :return: flipped image, updated bounding box coordinates\n",
        "    \"\"\"\n",
        "    # Flip image\n",
        "    new_image = FT.hflip(image)\n",
        "\n",
        "    # Flip boxes\n",
        "    new_boxes = boxes\n",
        "    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n",
        "    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n",
        "    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "def vertical_flip(image, boxes):\n",
        "    \"\"\"\n",
        "    Flip image vertically.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :return: flipped image, updated bounding box coordinates\n",
        "    \"\"\"\n",
        "    # Flip image\n",
        "    new_image = FT.vflip(image)\n",
        "\n",
        "    # Flip boxes\n",
        "    new_boxes = boxes\n",
        "    new_boxes[:, 1] = image.width - boxes[:, 1] - 1\n",
        "    new_boxes[:, 3] = image.width - boxes[:, 3] - 1\n",
        "    new_boxes = new_boxes[:, [0, 3, 2, 1]]\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "\n",
        "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
        "    \"\"\"\n",
        "    Resize image. For the SSD300, resize to (300, 300).\n",
        "\n",
        "    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n",
        "    you may choose to retain them.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n",
        "    \"\"\"\n",
        "    # Resize image\n",
        "    new_image = FT.resize(image, dims)\n",
        "\n",
        "    # Resize bounding boxes\n",
        "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
        "    new_boxes = boxes / old_dims  # percent coordinates\n",
        "\n",
        "    if not return_percent_coords:\n",
        "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
        "        new_boxes = new_boxes * new_dims\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "\n",
        "def photometric_distort(image, distort_type):\n",
        "    \"\"\"\n",
        "    Distort brightness, contrast, saturation, and hue, each with a 50% chance, in random order.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :return: distorted image\n",
        "    \"\"\"\n",
        "    new_image = image\n",
        "\n",
        "    if distort_type == 'hue':\n",
        "      adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
        "      new_image = FT.adjust_hue(new_image, adjust_factor)\n",
        "    elif distort_type == 'contrast':\n",
        "      adjust_factor = random.uniform(0.5, 1.5)\n",
        "      new_image = FT.adjust_contrast(new_image, adjust_factor)\n",
        "    elif distort_type == 'saturation':\n",
        "      adjust_factor = random.uniform(0.5, 1.5)\n",
        "      new_image = FT.adjust_saturation(new_image, adjust_factor)\n",
        "    elif distort_type == 'brightness':\n",
        "      adjust_factor = random.uniform(0.5, 1.5)\n",
        "      new_image = FT.adjust_brightness(new_image, adjust_factor)\n",
        "\n",
        "\n",
        "    # distortions = [FT.adjust_brightness,\n",
        "    #                FT.adjust_contrast,\n",
        "    #                FT.adjust_saturation,\n",
        "    #                FT.adjust_hue]\n",
        "\n",
        "    # random.shuffle(distortions)\n",
        "\n",
        "    # for d in distortions:\n",
        "    #     if random.random() < 0.5:\n",
        "    #         if d.__name__ is 'adjust_hue':\n",
        "    #             # Caffe repo uses a 'hue_delta' of 18 - we divide by 255 because PyTorch needs a normalized value\n",
        "    #             adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
        "    #         else:\n",
        "    #             # Caffe repo uses 'lower' and 'upper' values of 0.5 and 1.5 for brightness, contrast, and saturation\n",
        "    #             adjust_factor = random.uniform(0.5, 1.5)\n",
        "\n",
        "    #         # Apply this distortion\n",
        "    #         new_image = d(new_image, adjust_factor)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "\n",
        "def transform(image, boxes, labels, difficulties, split, augmentation):\n",
        "    \"\"\"\n",
        "    Apply the transformations above.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
        "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
        "    :param split: one of 'TRAIN' or 'TEST', since different sets of transformations are applied\n",
        "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
        "    \"\"\"\n",
        "    assert split in {'TRAIN', 'TEST'}\n",
        "\n",
        "    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n",
        "    # see: https://pytorch.org/docs/stable/torchvision/models.html\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    new_image = image\n",
        "    new_boxes = boxes\n",
        "    new_labels = labels\n",
        "    new_difficulties = difficulties\n",
        "    # Skip the following operations for evaluation/testing\n",
        "    if split == 'TRAIN':\n",
        "        # A series of photometric distortions in random order, each with 50% chance of occurrence, as in Caffe repo\n",
        "        if 'distort' in augmentation:\n",
        "          if 'hue' in augmentation:\n",
        "            new_image = photometric_distort(new_image, 'hue')\n",
        "          elif 'contrast' in augmentation:\n",
        "            new_image = photometric_distort(new_image, 'contrast')\n",
        "          elif 'saturation' in augmentation:\n",
        "            new_image = photometric_distort(new_image, 'saturation')\n",
        "          elif 'brightness' in augmentation:\n",
        "            new_image = photometric_distort(new_image, 'brightness')\n",
        "      \n",
        "        # Convert PIL image to Torch tensor\n",
        "        new_image = FT.to_tensor(new_image)\n",
        "\n",
        "        # Expand image (zoom out) with a 50% chance - helpful for training detection of small objects\n",
        "        # Fill surrounding space with the mean of ImageNet data that our base VGG was trained on\n",
        "        # if random.random() < 0.5:\n",
        "        #     new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
        "\n",
        "        # Randomly crop image (zoom in)\n",
        "        if augmentation == 'crop':\n",
        "          new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels, new_difficulties)\n",
        "\n",
        "        # Convert Torch tensor to PIL image\n",
        "        new_image = FT.to_pil_image(new_image)\n",
        "\n",
        "        # Flip image horizonatally\n",
        "        if augmentation == 'hflip':\n",
        "            new_image, new_boxes = horizontal_flip(new_image, new_boxes)\n",
        "        \n",
        "        # Flip image vertically\n",
        "        if augmentation == 'vflip':\n",
        "            new_image, new_boxes = vertical_flip(new_image, new_boxes)\n",
        "\n",
        "    # Resize image to (300, 300) - this also converts absolute boundary coordinates to their fractional form\n",
        "    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))\n",
        "\n",
        "    # Convert PIL image to Torch tensor\n",
        "    new_image = FT.to_tensor(new_image)\n",
        "\n",
        "    # Normalize by mean and standard deviation of ImageNet data that our base VGG was trained on\n",
        "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
        "\n",
        "    return new_image, new_boxes, new_labels, new_difficulties\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, scale):\n",
        "    \"\"\"\n",
        "    Scale learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param scale: factor to multiply learning rate with.\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * scale\n",
        "    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n",
        "\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "\n",
        "    :param epoch: epoch number\n",
        "    :param model: model\n",
        "    :param optimizer: optimizer\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer}\n",
        "    filename = '/content/drive/Shareddrives/딥러닝팀플/a-PyTorch-Tutorial-to-Object-Detection-master/trained_checkpoint_ssd300.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "metadata": {
        "id": "laosHXgLwcPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data lists"
      ],
      "metadata": {
        "id": "2CbgBur9w3iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = './output'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "create_data_lists(new_path='./new_dataset',\n",
        "                  output_folder=output_path)"
      ],
      "metadata": {
        "id": "5Nx6EiCnw5x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets.py"
      ],
      "metadata": {
        "id": "Ko-l9C142wSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class PascalVOCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, split, keep_difficult=False, augmentation='normal'):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param split: split, one of 'TRAIN' or 'TEST'\n",
        "        :param keep_difficult: keep or discard objects that are considered difficult to detect?\n",
        "        \"\"\"\n",
        "        self.split = split.upper()\n",
        "\n",
        "        assert self.split in {'TRAIN', 'TEST'}\n",
        "\n",
        "        self.data_folder = data_folder\n",
        "        self.keep_difficult = keep_difficult\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "        # Read data files\n",
        "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
        "            self.images = json.load(j)\n",
        "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
        "            self.objects = json.load(j)\n",
        "\n",
        "        assert len(self.images) == len(self.objects)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Read image\n",
        "        image = Image.open(self.images[i], mode='r')\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
        "        objects = self.objects[i]\n",
        "        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n",
        "        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n",
        "        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\n",
        "\n",
        "        # Discard difficult objects, if desired\n",
        "        if not self.keep_difficult:\n",
        "            boxes = boxes[1 - difficulties]\n",
        "            labels = labels[1 - difficulties]\n",
        "            difficulties = difficulties[1 - difficulties]\n",
        "\n",
        "        # Apply transformations\n",
        "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split, augmentation=self.augmentation)\n",
        "\n",
        "        return image, boxes, labels, difficulties\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
        "\n",
        "        This describes how to combine these tensors of different sizes. We use lists.\n",
        "\n",
        "        Note: this need not be defined in this Class, can be standalone.\n",
        "\n",
        "        :param batch: an iterable of N sets from __getitem__()\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "        \"\"\"\n",
        "\n",
        "        images = list()\n",
        "        boxes = list()\n",
        "        labels = list()\n",
        "        difficulties = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            boxes.append(b[1])\n",
        "            labels.append(b[2])\n",
        "            difficulties.append(b[3])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
      ],
      "metadata": {
        "id": "wv9KzIFq22Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model.py"
      ],
      "metadata": {
        "id": "gXFm9uyr3F5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from math import sqrt\n",
        "from itertools import product as product\n",
        "import torchvision\n",
        "import torchsummary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class VGGBase(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG base convolutions to produce lower-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGBase, self).__init__()\n",
        "\n",
        "        # Standard convolutional layers in VGG16\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
        "\n",
        "        # Replacements for FC6 and FC7 in VGG16\n",
        "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
        "\n",
        "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "\n",
        "        # Load pretrained layers\n",
        "        self.load_pretrained_layers()\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: lower-level feature maps conv4_3 and conv7\n",
        "        \"\"\"\n",
        "        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n",
        "        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n",
        "        out = self.pool1(out)  # (N, 64, 150, 150)\n",
        "\n",
        "        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n",
        "        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n",
        "        out = self.pool2(out)  # (N, 128, 75, 75)\n",
        "\n",
        "        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n",
        "        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
        "\n",
        "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = out  # (N, 512, 38, 38)\n",
        "        out = self.pool4(out)  # (N, 512, 19, 19)\n",
        "\n",
        "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
        "        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
        "\n",
        "        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        # Lower-level feature maps\n",
        "        return conv4_3_feats, conv7_feats\n",
        "\n",
        "    def load_pretrained_layers(self):\n",
        "        \"\"\"\n",
        "        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
        "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
        "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
        "        However, the original VGG-16 does not contain the conv6 and con7 layers.\n",
        "        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n",
        "        \"\"\"\n",
        "        # Current state of base\n",
        "        state_dict = self.state_dict()\n",
        "        param_names = list(state_dict.keys())\n",
        "\n",
        "        # Pretrained VGG base\n",
        "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
        "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
        "\n",
        "        # Transfer conv. parameters from pretrained model to current model\n",
        "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
        "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
        "\n",
        "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
        "        # fc6\n",
        "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
        "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
        "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
        "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
        "        # fc7\n",
        "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
        "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
        "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
        "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
        "\n",
        "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
        "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
        "        # ...operating on the 2D image of size (C, H, W) without padding\n",
        "\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "        print(\"\\nLoaded base model.\\n\")\n",
        "\n",
        "\n",
        "class AuxiliaryConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Additional convolutions to produce higher-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryConvolutions, self).__init__()\n",
        "\n",
        "        # Auxiliary/additional convolutions on top of the VGG base\n",
        "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
        "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
        "\n",
        "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
        "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
        "\n",
        "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
        "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
        "\n",
        "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
        "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
        "\n",
        "        # Initialize convolutions' parameters\n",
        "        self.init_conv2d()\n",
        "\n",
        "    def init_conv2d(self):\n",
        "        \"\"\"\n",
        "        Initialize convolution parameters.\n",
        "        \"\"\"\n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "\n",
        "    def forward(self, conv7_feats):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param conv7_feats: lower-level conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
        "        :return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
        "        \"\"\"\n",
        "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
        "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
        "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
        "\n",
        "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
        "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
        "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
        "\n",
        "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
        "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
        "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
        "\n",
        "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
        "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
        "\n",
        "        # Higher-level feature maps\n",
        "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
        "\n",
        "\n",
        "class PredictionConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutions to predict class scores and bounding boxes using lower and higher-level feature maps.\n",
        "\n",
        "    The bounding boxes (locations) are predicted as encoded offsets w.r.t each of the 8732 prior (default) boxes.\n",
        "    See 'cxcy_to_gcxgcy' in utils.py for the encoding definition.\n",
        "\n",
        "    The class scores represent the scores of each object class in each of the 8732 bounding boxes located.\n",
        "    A high score for 'background' = no object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        \"\"\"\n",
        "        :param n_classes: number of different types of objects\n",
        "        \"\"\"\n",
        "        super(PredictionConvolutions, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Number of prior-boxes we are considering per position in each feature map\n",
        "        n_boxes = {'conv4_3': 4,\n",
        "                   'conv7': 6,\n",
        "                   'conv8_2': 6,\n",
        "                   'conv9_2': 6,\n",
        "                   'conv10_2': 4,\n",
        "                   'conv11_2': 4}\n",
        "        # 4 prior-boxes implies we use 4 different aspect ratios, etc.\n",
        "\n",
        "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
        "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
        "\n",
        "        # Class prediction convolutions (predict classes in localization boxes)\n",
        "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
        "\n",
        "        # Initialize convolutions' parameters\n",
        "        self.init_conv2d()\n",
        "\n",
        "    def init_conv2d(self):\n",
        "        \"\"\"\n",
        "        Initialize convolution parameters.\n",
        "        \"\"\"\n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "\n",
        "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
        "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
        "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
        "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
        "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
        "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
        "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
        "        \"\"\"\n",
        "        batch_size = conv4_3_feats.size(0)\n",
        "\n",
        "        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n",
        "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)  # (N, 16, 38, 38)\n",
        "        l_conv4_3 = l_conv4_3.permute(0, 2, 3,\n",
        "                                      1).contiguous()  # (N, 38, 38, 16), to match prior-box order (after .view())\n",
        "        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n",
        "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)  # (N, 5776, 4), there are a total 5776 boxes on this feature map\n",
        "\n",
        "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
        "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
        "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
        "\n",
        "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
        "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
        "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
        "\n",
        "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
        "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
        "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
        "\n",
        "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
        "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
        "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
        "\n",
        "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
        "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
        "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
        "\n",
        "        # Predict classes in localization boxes\n",
        "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
        "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
        "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
        "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
        "                                   self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
        "\n",
        "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
        "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
        "        c_conv7 = c_conv7.view(batch_size, -1,\n",
        "                               self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
        "\n",
        "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
        "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
        "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
        "\n",
        "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
        "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
        "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
        "\n",
        "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
        "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
        "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
        "\n",
        "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
        "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
        "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
        "\n",
        "        # A total of 8732 boxes\n",
        "        # Concatenate in this specific order (i.e. must match the order of the prior-boxes)\n",
        "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
        "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2],\n",
        "                                   dim=1)  # (N, 8732, n_classes)\n",
        "\n",
        "        return locs, classes_scores\n",
        "\n",
        "\n",
        "class SSD300(nn.Module):\n",
        "    \"\"\"\n",
        "    The SSD300 network - encapsulates the base VGG network, auxiliary, and prediction convolutions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(SSD300, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.base = VGGBase()\n",
        "        self.aux_convs = AuxiliaryConvolutions()\n",
        "        self.pred_convs = PredictionConvolutions(n_classes)\n",
        "\n",
        "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
        "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "        # Prior boxes\n",
        "        self.priors_cxcy = self.create_prior_boxes()\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
        "        \"\"\"\n",
        "        # Run VGG base network convolutions (lower level feature map generators)\n",
        "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
        "\n",
        "        # Rescale conv4_3 after L2 norm\n",
        "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
        "        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n",
        "\n",
        "        # Run auxiliary convolutions (higher level feature map generators)\n",
        "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
        "            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
        "\n",
        "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
        "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
        "                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        return locs, classes_scores\n",
        "\n",
        "    def create_prior_boxes(self):\n",
        "        \"\"\"\n",
        "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
        "\n",
        "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
        "        \"\"\"\n",
        "        fmap_dims = {'conv4_3': 38,\n",
        "                     'conv7': 19,\n",
        "                     'conv8_2': 10,\n",
        "                     'conv9_2': 5,\n",
        "                     'conv10_2': 3,\n",
        "                     'conv11_2': 1}\n",
        "\n",
        "        obj_scales = {'conv4_3': 0.1,\n",
        "                      'conv7': 0.2,\n",
        "                      'conv8_2': 0.375,\n",
        "                      'conv9_2': 0.55,\n",
        "                      'conv10_2': 0.725,\n",
        "                      'conv11_2': 0.9}\n",
        "\n",
        "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
        "                         'conv7': [1., 2., 3., 0.5, .333],\n",
        "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
        "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
        "                         'conv10_2': [1., 2., 0.5],\n",
        "                         'conv11_2': [1., 2., 0.5]}\n",
        "\n",
        "        fmaps = list(fmap_dims.keys())\n",
        "\n",
        "        prior_boxes = []\n",
        "\n",
        "        for k, fmap in enumerate(fmaps):\n",
        "            for i in range(fmap_dims[fmap]):\n",
        "                for j in range(fmap_dims[fmap]):\n",
        "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
        "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
        "\n",
        "                    for ratio in aspect_ratios[fmap]:\n",
        "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
        "\n",
        "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
        "                        # scale of the current feature map and the scale of the next feature map\n",
        "                        if ratio == 1.:\n",
        "                            try:\n",
        "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
        "                            # For the last feature map, there is no \"next\" feature map\n",
        "                            except IndexError:\n",
        "                                additional_scale = 1.\n",
        "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
        "\n",
        "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
        "        prior_boxes.clamp_(0, 1)  # (8732, 4)\n",
        "\n",
        "        return prior_boxes\n",
        "\n",
        "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
        "        \"\"\"\n",
        "        Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.\n",
        "\n",
        "        For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.\n",
        "\n",
        "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
        "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
        "        :param min_score: minimum threshold for a box to be considered a match for a certain class\n",
        "        :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS\n",
        "        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
        "        :return: detections (boxes, labels, and scores), lists of length batch_size\n",
        "        \"\"\"\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n",
        "\n",
        "        # Lists to store final predicted boxes, labels, and scores for all images\n",
        "        all_images_boxes = list()\n",
        "        all_images_labels = list()\n",
        "        all_images_scores = list()\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Decode object coordinates from the form we regressed predicted boxes to\n",
        "            decoded_locs = cxcy_to_xy(\n",
        "                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n",
        "\n",
        "            # Lists to store boxes and scores for this image\n",
        "            image_boxes = list()\n",
        "            image_labels = list()\n",
        "            image_scores = list()\n",
        "\n",
        "            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n",
        "\n",
        "            # Check for each class\n",
        "            for c in range(1, self.n_classes):\n",
        "                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n",
        "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
        "                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n",
        "                n_above_min_score = score_above_min_score.sum().item()\n",
        "                if n_above_min_score == 0:\n",
        "                    continue\n",
        "                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n",
        "                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n",
        "\n",
        "                # Sort predicted boxes and scores by scores\n",
        "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
        "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
        "\n",
        "                # Find the overlap between predicted boxes\n",
        "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n",
        "\n",
        "                # Non-Maximum Suppression (NMS)\n",
        "\n",
        "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
        "                # 1 implies suppress, 0 implies don't suppress\n",
        "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
        "\n",
        "                # Consider each box in order of decreasing scores\n",
        "                for box in range(class_decoded_locs.size(0)):\n",
        "                    # If this box is already marked for suppression\n",
        "                    if suppress[box] == 1:\n",
        "                        continue\n",
        "\n",
        "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
        "                    # Find such boxes and update suppress indices\n",
        "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
        "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
        "\n",
        "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
        "                    suppress[box] = 0\n",
        "\n",
        "                # Store only unsuppressed boxes for this class\n",
        "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
        "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
        "                image_scores.append(class_scores[1 - suppress])\n",
        "\n",
        "            # If no object in any class is found, store a placeholder for 'background'\n",
        "            if len(image_boxes) == 0:\n",
        "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
        "                image_labels.append(torch.LongTensor([0]).to(device))\n",
        "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
        "\n",
        "            # Concatenate into single tensors\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
        "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
        "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
        "            n_objects = image_scores.size(0)\n",
        "\n",
        "            # Keep only the top k objects\n",
        "            if n_objects > top_k:\n",
        "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
        "                image_scores = image_scores[:top_k]  # (top_k)\n",
        "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
        "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
        "\n",
        "            # Append to lists that store predicted boxes and scores for all images\n",
        "            all_images_boxes.append(image_boxes)\n",
        "            all_images_labels.append(image_labels)\n",
        "            all_images_scores.append(image_scores)\n",
        "\n",
        "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size\n",
        "\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The MultiBox loss, a loss function for object detection.\n",
        "\n",
        "    This is a combination of:\n",
        "    (1) a localization loss for the predicted locations of the boxes, and\n",
        "    (2) a confidence loss for the predicted class scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.priors_cxcy = priors_cxcy\n",
        "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
        "        self.threshold = threshold\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.smooth_l1 = nn.L1Loss()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
        "\n",
        "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
        "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
        "        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n",
        "        :param labels: true object labels, a list of N tensors\n",
        "        :return: multibox loss, a scalar\n",
        "        \"\"\"\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        n_classes = predicted_scores.size(2)\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
        "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n",
        "\n",
        "        # For each image\n",
        "        for i in range(batch_size):\n",
        "            n_objects = boxes[i].size(0)\n",
        "\n",
        "            overlap = find_jaccard_overlap(boxes[i],\n",
        "                                           self.priors_xy)  # (n_objects, 8732)\n",
        "\n",
        "            # For each prior, find the object that has the maximum overlap\n",
        "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n",
        "\n",
        "            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n",
        "            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n",
        "            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n",
        "\n",
        "            # To remedy this -\n",
        "            # First, find the prior that has the maximum overlap for each object.\n",
        "            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n",
        "\n",
        "            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n",
        "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
        "\n",
        "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
        "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
        "\n",
        "            # Labels for each prior\n",
        "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
        "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
        "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
        "\n",
        "            # Store\n",
        "            true_classes[i] = label_for_each_prior\n",
        "\n",
        "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
        "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
        "\n",
        "        # Identify priors that are positive (object/non-background)\n",
        "        positive_priors = true_classes != 0  # (N, 8732)\n",
        "\n",
        "        # LOCALIZATION LOSS\n",
        "\n",
        "        # Localization loss is computed only over positive (non-background) priors\n",
        "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
        "\n",
        "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
        "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
        "\n",
        "        # CONFIDENCE LOSS\n",
        "\n",
        "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
        "        # That is, FOR EACH IMAGE,\n",
        "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
        "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
        "\n",
        "        # Number of positive and hard-negative priors per image\n",
        "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
        "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
        "\n",
        "        # First, find the loss for all priors\n",
        "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
        "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
        "\n",
        "        # We already know which priors are positive\n",
        "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
        "\n",
        "        # Next, find which priors are hard-negative\n",
        "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
        "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
        "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
        "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
        "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
        "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
        "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
        "\n",
        "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
        "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
        "\n",
        "        # TOTAL LOSS\n",
        "\n",
        "        return conf_loss + self.alpha * loc_loss\n",
        "\n",
        "# model = SSD300(n_classes=7)\n",
        "# torchsummary.summary(model, input_size=(3, 300, 300), batch_size=8, device='cpu')\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "AHcm6MHY3JW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train.py"
      ],
      "metadata": {
        "id": "PU0dbI1W2hD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "# Data parameters\n",
        "data_folder = './output'  # folder with data files\n",
        "keep_difficult = True  # use objects considered difficult to detect?\n",
        "\n",
        "# Model parameters\n",
        "# Not too many here since the SSD300 has a very specific structure\n",
        "n_classes = len(label_map)  # number of different types of objects\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Learning parameters\n",
        "# checkpoint = None\n",
        "checkpoint = '/content/drive/Shareddrives/딥러닝팀플/a-PyTorch-Tutorial-to-Object-Detection-master/checkpoint_ssd300.pth.tar'  # path to model checkpoint, None if none (/content/drive/MyDrive/deep/a-PyTorch-Tutorial-to-Object-Detection-master/checkpoint_ssd300.pth.tar)\n",
        "batch_size = 8  # batch size\n",
        "# iterations = 120000  # number of iterations to train\n",
        "iterations = 60000\n",
        "workers = 4  # number of workers for loading data in the DataLoader\n",
        "print_freq = 100  # print training status every __ batches\n",
        "lr = 1e-3  # learning rate\n",
        "decay_lr_at = [40000, 50000]  # decay learning rate after these many iterations\n",
        "decay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\n",
        "momentum = 0.9  # momentum\n",
        "weight_decay = 5e-4  # weight decay\n",
        "grad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Training.\n",
        "    \"\"\"\n",
        "    global start_epoch, label_map, epoch, checkpoint, decay_lr_at\n",
        "\n",
        "    # Initialize model or load checkpoint\n",
        "    if checkpoint is None:\n",
        "        start_epoch = 0\n",
        "        model = SSD300(n_classes=n_classes)\n",
        "        # Initialize the optimizer, with twice the default learning rate for biases, as in the original Caffe repo\n",
        "        biases = list()\n",
        "        not_biases = list()\n",
        "        for param_name, param in model.named_parameters():\n",
        "            print(param_name, param)\n",
        "            if param.requires_grad:\n",
        "                if param_name.endswith('.bias'):\n",
        "                    biases.append(param)\n",
        "                else:\n",
        "                    not_biases.append(param)\n",
        "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
        "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint, map_location=\"\")\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
        "        model = checkpoint['model']\n",
        "        # optimizer = checkpoint['optimizer']\n",
        "        biases = list()\n",
        "        not_biases = list()\n",
        "        params_to_update = []\n",
        "        for param_name, param in model.named_parameters():\n",
        "            print(param_name)\n",
        "            param.requires_grad = False\n",
        "            if ('aux_convs' in param_name or 'pred_convs.' in param_name):\n",
        "                param.requires_grad = True\n",
        "                if param_name.endswith('.bias'):\n",
        "                    biases.append(param)\n",
        "                else:\n",
        "                    not_biases.append(param)\n",
        "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
        "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    # Move to default device\n",
        "    model = model.to(device)\n",
        "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
        "\n",
        "    # Custom dataloaders\n",
        "    total_train_dataset = PascalVOCDataset(data_folder,\n",
        "                                    split='train',\n",
        "                                    keep_difficult=keep_difficult,\n",
        "                                    augmentation='normal')\n",
        "\n",
        "    augmentation_list = ['hflip', 'vflip', 'crop', 'distort_hue', 'distort_contrast', 'distort_saturation', 'distort_brightness']\n",
        "    for aug in augmentation_list:\n",
        "      train_dataset = PascalVOCDataset(data_folder,\n",
        "                                      split='train',\n",
        "                                      keep_difficult=keep_difficult,\n",
        "                                      augmentation=aug)\n",
        "      total_train_dataset += train_dataset\n",
        "\n",
        "    print('Train dataset size: ', len(total_train_dataset))\n",
        "    train_loader = torch.utils.data.DataLoader(total_train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
        "                                               pin_memory=True)  # note that we're passing the collate function here\n",
        "\n",
        "    # Calculate total number of epochs to train and the epochs to decay learning rate at (i.e. convert iterations to epochs)\n",
        "    # To convert iterations to epochs, divide iterations by the number of iterations per epoch\n",
        "    # The paper trains for 120,000 iterations with a batch size of 32, decays after 80,000 and 100,000 iterations\n",
        "    epochs = iterations // (len(train_dataset) // 16)\n",
        "    decay_lr_at = [it // (len(train_dataset) // 16) for it in decay_lr_at]\n",
        "\n",
        "    # Epochs\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        epoch_time = time.time()\n",
        "        # Decay learning rate at particular epochs\n",
        "        if epoch in decay_lr_at:\n",
        "            adjust_learning_rate(optimizer, decay_lr_to)\n",
        "\n",
        "        # One epoch's training\n",
        "        train(train_loader=train_loader,\n",
        "              model=model,\n",
        "              criterion=criterion,\n",
        "              optimizer=optimizer,\n",
        "              epoch=epoch)\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(epoch, model, optimizer)\n",
        "        print('Time per epoch: ', (time.time() - epoch_time) / 60)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    One epoch's training.\n",
        "\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param model: model\n",
        "    :param criterion: MultiBox loss\n",
        "    :param optimizer: optimizer\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "    model.train()  # training mode enables dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to default device\n",
        "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
        "        boxes = [b.to(device) for b in boxes]\n",
        "        labels = [l.to(device) for l in labels]\n",
        "\n",
        "        # Forward prop.\n",
        "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
        "\n",
        "        # Backward prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients, if necessary\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "\n",
        "        # Update model\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
        "                                                                  batch_time=batch_time,\n",
        "                                                                  data_time=data_time, loss=losses))\n",
        "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "b0H9XcRU2iR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval.py"
      ],
      "metadata": {
        "id": "uV2u-eID8aKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from utils import *\n",
        "# from datasets import PascalVOCDataset\n",
        "from tqdm import tqdm\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "# Good formatting when printing the APs for each class and mAP\n",
        "pp = PrettyPrinter()\n",
        "\n",
        "# Parameters\n",
        "data_folder = './output'\n",
        "keep_difficult = True  # difficult ground truth objects must always be considered in mAP calculation, because these objects DO exist!\n",
        "batch_size = 64\n",
        "workers = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = '/content/drive/Shareddrives/딥러닝팀플/a-PyTorch-Tutorial-to-Object-Detection-master/trained_checkpoint_ssd300.pth.tar'\n",
        "\n",
        "# Load model checkpoint that is to be evaluated\n",
        "checkpoint = torch.load(checkpoint)\n",
        "model = checkpoint['model']\n",
        "model = model.to(device)\n",
        "\n",
        "# Switch to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Load test data\n",
        "test_dataset = PascalVOCDataset(data_folder,\n",
        "                                split='test',\n",
        "                                keep_difficult=keep_difficult)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
        "                                          collate_fn=test_dataset.collate_fn, num_workers=workers, pin_memory=True)\n",
        "\n",
        "\n",
        "def evaluate(test_loader, model):\n",
        "    \"\"\"\n",
        "    Evaluate.\n",
        "\n",
        "    :param test_loader: DataLoader for test data\n",
        "    :param model: model\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure it's in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store detected and true boxes, labels, scores\n",
        "    det_boxes = list()\n",
        "    det_labels = list()\n",
        "    det_scores = list()\n",
        "    true_boxes = list()\n",
        "    true_labels = list()\n",
        "    true_difficulties = list()  # it is necessary to know which objects are 'difficult', see 'calculate_mAP' in utils.py\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
        "            images = images.to(device)  # (N, 3, 300, 300)\n",
        "\n",
        "            # Forward prop.\n",
        "            predicted_locs, predicted_scores = model(images)\n",
        "\n",
        "            # Detect objects in SSD output\n",
        "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
        "                                                                                       min_score=0.01, max_overlap=0.45,\n",
        "                                                                                       top_k=200)\n",
        "            # Evaluation MUST be at min_score=0.01, max_overlap=0.45, top_k=200 for fair comparision with the paper's results and other repos\n",
        "\n",
        "            # Store this batch's results for mAP calculation\n",
        "            boxes = [b.to(device) for b in boxes]\n",
        "            labels = [l.to(device) for l in labels]\n",
        "            difficulties = [d.to(device) for d in difficulties]\n",
        "\n",
        "            det_boxes.extend(det_boxes_batch)\n",
        "            det_labels.extend(det_labels_batch)\n",
        "            det_scores.extend(det_scores_batch)\n",
        "            true_boxes.extend(boxes)\n",
        "            true_labels.extend(labels)\n",
        "            true_difficulties.extend(difficulties)\n",
        "\n",
        "        # Calculate mAP\n",
        "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
        "\n",
        "    # Print AP for each class\n",
        "    pp.pprint(APs)\n",
        "\n",
        "    print('\\nMean Average Precision (mAP): %.3f' % mAP)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    evaluate(test_loader, model) "
      ],
      "metadata": {
        "id": "88Xji5Zk8dyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vXyI6n82TsDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detect.py"
      ],
      "metadata": {
        "id": "gCtskWsFFQb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "# from utils import *\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model checkpoint\n",
        "checkpoint = '/content/drive/Shareddrives/딥러닝팀플/a-PyTorch-Tutorial-to-Object-Detection-master/trained_checkpoint_ssd300.pth.tar'\n",
        "checkpoint = torch.load(checkpoint)\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
        "model = checkpoint['model']\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Transforms\n",
        "resize = transforms.Resize((300, 300))\n",
        "to_tensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "def detect(original_image, min_score, max_overlap, top_k, suppress=None):\n",
        "    \"\"\"\n",
        "    Detect objects in an image with a trained SSD300, and visualize the results.\n",
        "\n",
        "    :param original_image: image, a PIL Image\n",
        "    :param min_score: minimum threshold for a detected box to be considered a match for a certain class\n",
        "    :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via Non-Maximum Suppression (NMS)\n",
        "    :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
        "    :param suppress: classes that you know for sure cannot be in the image or you do not want in the image, a list\n",
        "    :return: annotated image, a PIL Image\n",
        "    \"\"\"\n",
        "\n",
        "    # Transform\n",
        "    image = normalize(to_tensor(resize(original_image)))\n",
        "\n",
        "    # Move to default device\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Forward prop.\n",
        "    predicted_locs, predicted_scores = model(image.unsqueeze(0))\n",
        "\n",
        "    # Detect objects in SSD output\n",
        "    det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=min_score,\n",
        "                                                             max_overlap=max_overlap, top_k=top_k)\n",
        "\n",
        "    # Move detections to the CPU\n",
        "    det_boxes = det_boxes[0].to('cpu')\n",
        "\n",
        "    # Transform to original image dimensions\n",
        "    original_dims = torch.FloatTensor(\n",
        "        [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
        "    det_boxes = det_boxes * original_dims\n",
        "\n",
        "    # Decode class integer labels\n",
        "    det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
        "\n",
        "    # If no objects found, the detected labels will be set to ['0.'], i.e. ['background'] in SSD300.detect_objects() in model.py\n",
        "    if det_labels == ['background']:\n",
        "        # Just return original image\n",
        "        return original_image\n",
        "\n",
        "    # Annotate\n",
        "    annotated_image = original_image\n",
        "    draw = ImageDraw.Draw(annotated_image)\n",
        "    font = ImageFont.truetype(\"/content/drive/Shareddrives/딥러닝팀플/a-PyTorch-Tutorial-to-Object-Detection-master/calibril.ttf\", 15)\n",
        "\n",
        "    # Suppress specific classes, if needed\n",
        "    for i in range(det_boxes.size(0)):\n",
        "        if suppress is not None:\n",
        "            if det_labels[i] in suppress:\n",
        "                continue\n",
        "\n",
        "        # Boxes\n",
        "        box_location = det_boxes[i].tolist()\n",
        "        draw.rectangle(xy=box_location, outline=label_color_map[det_labels[i]])\n",
        "        draw.rectangle(xy=[l + 1. for l in box_location], outline=label_color_map[\n",
        "            det_labels[i]])  # a second rectangle at an offset of 1 pixel to increase line thickness\n",
        "        # draw.rectangle(xy=[l + 2. for l in box_location], outline=label_color_map[\n",
        "        #     det_labels[i]])  # a third rectangle at an offset of 1 pixel to increase line thickness\n",
        "        # draw.rectangle(xy=[l + 3. for l in box_location], outline=label_color_map[\n",
        "        #     det_labels[i]])  # a fourth rectangle at an offset of 1 pixel to increase line thickness\n",
        "\n",
        "        # Text\n",
        "        text_size = font.getsize(det_labels[i].upper())\n",
        "        text_location = [box_location[0] + 2., box_location[1] - text_size[1]]\n",
        "        textbox_location = [box_location[0], box_location[1] - text_size[1], box_location[0] + text_size[0] + 4.,\n",
        "                            box_location[1]]\n",
        "        draw.rectangle(xy=textbox_location, fill=label_color_map[det_labels[i]])\n",
        "        draw.text(xy=text_location, text=det_labels[i].upper(), fill='white',\n",
        "                  font=font)\n",
        "    del draw\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    img_path = '/content/new_dataset/JPEGImages/000055.jpg'\n",
        "    original_image = Image.open(img_path, mode='r')\n",
        "    original_image = original_image.convert('RGB')\n",
        "    img = detect(original_image, min_score=0.2, max_overlap=0.5, top_k=200)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(img)"
      ],
      "metadata": {
        "id": "dpAApssKFSHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Xf6M3mpUo0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        " \n",
        "img_array = []\n",
        "for filename in glob.glob('/content/drive/Shareddrives/imu/case1/RgbPng(173318)/*.png'): #directory name/*.png\n",
        "    img = cv2.imread(filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    img_array.append(img)\n",
        "img_array.sort()\n",
        " \n",
        " \n",
        "out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, size) # 30 : frame rate\n",
        " \n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()"
      ],
      "metadata": {
        "id": "fDZuNLVLVGMN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}